{
  "hash": "37a1279814f2c16bbb50d469cd5844a8",
  "result": {
    "markdown": "---\ntitle: \"ESOC 2023: Predicting physical activity level after stroke\"\ndate: 2023-05-03\nexecute: \n  freeze: true\nimage: images/esoc2023.png\ndescription: A little background to our abstract presented at the ESOC 2023 conference in Munich. \ncategories:\n  - Conference abstract\n  - Physical activity\n  - Elastic net\n  - R\n---\n\n\n## Intro\n\nI am presenting a poster at European Stroke Organisation Conference 2023 on predicting changes in physical activity after stroke.\n\nThe poster will be part of the [poster viewing programme on Wednesday, May 24 2023](https://apps.congrex.com/esoc2023/en-GB/pag/faculty/1149907).\n\n## About the poster\n\nSee the [poster here.](images/esoc2023poster.pdf)\n\nI wanted to divert from the traditional text heavy poster format. For the session at ESOC, I will be at the poster stand for most of the time to talk about our work. The abstract will be available for download for the participants.\n\nThe poster is created in PowerPoint, as this was where I had an available template. The template was later abandoned though. The font used is the free and open source font [Jost\\*](https://github.com/indestructible-type/Jost). Inspired by German design tradition. Icons are from the [Material Design Icons](https://pictogrammers.com/library/mdi/), and also open source.\n\n## Background\n\nPhysical activity (PA) reduces the risk of stroke and improves functional outcome. We aimed to investigate predictors for decrease and increase in PA after stroke. We have been interested in trying to predict patients at increased risk of physical activity decline after stroke.\n\n## Methods\n\nAll analysis were performed using R and RStudio. We used the elastic net regression model as implemented in the [`glmnet`-package](https://glmnet.stanford.edu/articles/glmnet.html) for R.[^1]\n\n[^1]: Versions of `glmnet` also exists for MATLAB and Python\n\nI have used the great book \"An introduction to statistical learning with applications in R\".[@jamesIntroductionStatisticalLearning2021] This book [is freely available](https://hastie.su.domains/ISLR2/ISLRv2_website.pdf) and the authors have even created small talks on each chapter (though only for the first edition). I believe this book is the main curriculum for beginning work with statistical learning (or machine learning, but that matter).\n\nThe script used for creating a regularised prediction model is below.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-summary=\"Optimisation and regularisation steps\"}\n## ====================================================================\n## Step 0: data import and wrangling\n## ====================================================================\n\n# source(\"data_format.R\")\ny1<-factor(as.integer(y)-1) ## Outcome is required to be factor of 0 or 1.\n\n\n## ====================================================================\n## Step 1: settings\n## ====================================================================\n\n## Folds\nK=10\nset.seed(3)\nc<-caret::createFolds(y=y, \n                      k = K, \n                      list = FALSE, \n                      returnTrain = TRUE) # Foldids for alpha tuning\n\n## Defining tuning parameters\nlambdas=2^seq(-10, 5, 1)\nalphas<-seq(0,1,.1)\n\n## Weights for models\nweighted=TRUE\nif (weighted == TRUE) {\n  wght<-as.vector(1 - (table(y)[y] / length(y)))\n} else {\n  wght <- rep(1, nrow(y))\n}\n\n\n## Standardise numeric\n## Centered and \n\n\n\n## ====================================================================\n## Step 2: all cross validations for each alpha\n## ====================================================================\n\nlibrary(furrr)\nlibrary(purrr)\nlibrary(doMC)\nregisterDoMC(cores=6)\n\n# Nested CVs with analysis for all lambdas for each alpha\n# \nset.seed(3)\ncvs <- future_map(alphas, function(a){\n  cv.glmnet(model.matrix(~.-1,X),\n            y1,\n            weights = wght,\n            lambda=lambdas, \n            type.measure = \"deviance\", # This is standard measure and recommended for tuning\n            foldid = c, # Per recommendation the folds are kept for alpha optimisation\n            alpha=a,\n            standardize=TRUE,\n            family=quasibinomial,\n            keep=TRUE) # Same as binomial, but not as picky\n})\n\n## ====================================================================\n# Step 3: optimum lambda for each alpha\n## ====================================================================\n\n\n# For each alpha, lambda is chosen for the lowest meassure (deviance)\neach_alpha <- sapply(seq_along(alphas), function(id) {\n  each_cv <- cvs[[id]]\n  alpha_val <- alphas[id]\n  index_lmin <- match(each_cv$lambda.min, \n                      each_cv$lambda)\n  c(lamb = each_cv$lambda.min, \n    alph = alpha_val,\n    cvm = each_cv$cvm[index_lmin])\n})\n\n# Best lambda\nbest_lamb <- min(each_alpha[\"lamb\", ])\n\n# Alpha is chosen for best lambda with lowest model deviance, each_alpha[\"cvm\",]\nbest_alph <- each_alpha[\"alph\",][each_alpha[\"cvm\",]==min(each_alpha[\"cvm\",]\n                                                         [each_alpha[\"lamb\",] %in% best_lamb])]\n\n## https://stackoverflow.com/questions/42007313/plot-an-roc-curve-in-r-with-ggplot2\np_roc<-roc.glmnet(cvs[[1]]$fit.preval, newy = y)[[match(best_alph,alphas)]]|> # Plots performance from model with best alpha\n  ggplot(aes(FPR,TPR)) + \n  geom_step() +\n  coord_cartesian(xlim=c(0,1), ylim=c(0,1)) +\n  geom_abline()+\n  theme_bw()\n\n## ====================================================================\n# Step 4: Creating the final model\n## ====================================================================\n\nsource(\"regular_fun.R\") # Custom function\noptimised_model<-regular_fun(X,y1,K,lambdas=best_lamb,alpha=best_alph) \n# With lambda and alpha specified, the function is just a k-fold cross-validation wrapper, \n# but keeps model performance figures from each fold.\n\nlist2env(optimised_model,.GlobalEnv)\n# Function outputs a list, which is unwrapped to Env.\n# See source script for reference.\n\n## ====================================================================\n# Step 5: creating table of coefficients for inference\n## ====================================================================\n\nBmatrix<-matrix(unlist(B),ncol=10)\nBmedian<-apply(Bmatrix,1,median)\nBmean<-apply(Bmatrix,1,mean)\n\nreg_coef_tbl<-tibble(\n  name = c(\"Intercept\",Hmisc::label(X)),\n  medianX = round(Bmedian,5),\n  ORmed = round(exp(Bmedian),5),\n  meanX = round(Bmean,5),\n  ORmea = round(exp(Bmean),5))%>%\n  # arrange(desc(abs(medianX)))%>%\n  gt()\n\n## ====================================================================\n# Step 6: plotting predictive performance\n## ====================================================================\n\nreg_cfm<-confusionMatrix(cMatTest)\nreg_auc_sum<-summary(auc_test[,1])\n\n## ====================================================================\n# Step 7: Packing list to save in loop\n## ====================================================================\n\nls[[i]] <- list(\"RegularisedCoefs\"=reg_coef_tbl,\n                \"bestA\"=best_alph,\n                \"bestL\"=best_lamb,\n                \"ConfusionMatrx\"=reg_cfm,\n                \"AUROC\"=reg_auc_sum)\n```\n:::\n\n\n## Publication status\n\nWe have recently applied for additional registry based data on socio economic status and educational level to include in the analysis. We are awaiting this data before publishing our main article on this project.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}