[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome!",
    "section": "",
    "text": "Hi! I am Andreas Gammelgaard Damsbo. I am a medical doctor and PhD-student at the Danish Stroke Centre, Aarhus University Hospital and Department of Clinical Medicine, Aarhus University.\nThis is my professional website, where I will write about the research and other projects I am engaged with through my work. Don’t hesitate to contact me if you want to collaborate or want to share your thoughts on some of my work.\nI use open source software in my work as much as possible, and document my work following the same ideology for others to get inspired or criticise. Likewise, the source for this page is freely available on GitHub.\nI also do other things than work, which I might link to or write about.\nTake care!"
  },
  {
    "objectID": "health_data/ancle.html",
    "href": "health_data/ancle.html",
    "title": "Project: Foot database",
    "section": "",
    "text": "This project is the most comprehensive data base migration yet.\nThe client’s wish, was for an old Microsoft Access data base to be migrated to REDCap, for continous use and data collection."
  },
  {
    "objectID": "health_data/ancle.html#background",
    "href": "health_data/ancle.html#background",
    "title": "Project: Foot database",
    "section": "",
    "text": "This project is the most comprehensive data base migration yet.\nThe client’s wish, was for an old Microsoft Access data base to be migrated to REDCap, for continous use and data collection."
  },
  {
    "objectID": "health_data/ancle.html#database",
    "href": "health_data/ancle.html#database",
    "title": "Project: Foot database",
    "section": "Database",
    "text": "Database\nHere are a few characteristics:\n\nAround 3000 entries\n5 collection instruments, 4 of them were to be used as repeatable instruments.\n80 variables"
  },
  {
    "objectID": "health_data/ancle.html#coding",
    "href": "health_data/ancle.html#coding",
    "title": "Project: Foot database",
    "section": "Coding",
    "text": "Coding\nI have collected my scripts in a repository, which will be made public when I have finished the work."
  },
  {
    "objectID": "health_data/ancle.html#extend-of-the-work",
    "href": "health_data/ancle.html#extend-of-the-work",
    "title": "Project: Foot database",
    "section": "Extend of the work",
    "text": "Extend of the work"
  },
  {
    "objectID": "health_data/stRoke.html",
    "href": "health_data/stRoke.html",
    "title": "Package: stRoke",
    "section": "",
    "text": "During my early use of R, I started to collect my own solutions to problems in an package. At some point I realised that most of the problems were already solved elsewhere and typically more elegantly. I took the best functions and moved them to a new project, the stRoke-package.(Damsbo 2023) This package have been used for learning and practising, but it has also been published to CRAN and I will keep including new functions in the package.\nThe main goal has been learning, but I also wanted to share my work for others to use or modify."
  },
  {
    "objectID": "health_data/stRoke.html#my-first-package",
    "href": "health_data/stRoke.html#my-first-package",
    "title": "Package: stRoke",
    "section": "",
    "text": "During my early use of R, I started to collect my own solutions to problems in an package. At some point I realised that most of the problems were already solved elsewhere and typically more elegantly. I took the best functions and moved them to a new project, the stRoke-package.(Damsbo 2023) This package have been used for learning and practising, but it has also been published to CRAN and I will keep including new functions in the package.\nThe main goal has been learning, but I also wanted to share my work for others to use or modify."
  },
  {
    "objectID": "health_data/stRoke.html#content-of-the-stroke-package",
    "href": "health_data/stRoke.html#content-of-the-stroke-package",
    "title": "Package: stRoke",
    "section": "Content of the stRoke package",
    "text": "Content of the stRoke package\nThe different functions have been documented in the package vignette. Here I will just discuss a few favourite examples.\n\nPlotting text with contrast based on background\nThe first example is just a small exercise in implementing ideas as a function.\nInspired by a discussion on StackOverflow and an example, I created a function to determine contrast levels. My use case was colouring text based on background colour, but the function can be used in many other cases.\nSimilar functionality is used in Polychrome::swatch() or scales::show_col(), but the function used for determining contrast here is a little crude. The contrast_text() is a little more sophisticated, and can be used on its own. Is it necessary? You’ll decide for yourself.\n\nlibrary(stRoke)\ncontrast_text(c(\"red\",\"yellow\",\"blue\",\"green\",\"purple\", \"orange\",\"white\",\"black\"))\n\n[1] \"white\" \"black\" \"white\" \"black\" \"white\" \"white\" \"black\" \"white\"\n\n\nTo give an example, I have modified the scales::show_col().\n\n\nA modification of the scales::show_col() function to use stRoke::contrast_text() for text coloring.\n## Operator used by library(scales), but not exported\n## Defined for convenience. It is a neat operator.\n`%||%` &lt;- function(a,b){\n  if (is.null(a)) b else a\n}\n\n## Modified color_plot() function\ncolor_plot &lt;-\n  function (colours,\n            labels = TRUE,\n            borders = NULL,\n            cex_label = 1,\n            ncol = NULL,\n            ...)\n  {\n    n &lt;- length(colours)\n    ncol &lt;- ncol %||% ceiling(sqrt(length(colours)))\n    nrow &lt;- ceiling(n / ncol)\n    colours &lt;- c(colours, rep(NA, nrow * ncol - length(colours)))\n    colours &lt;- matrix(colours, ncol = ncol, byrow = TRUE)\n    old &lt;- par(pty = \"s\", mar = c(0, 0, 0, 0))\n    on.exit(par(old))\n    size &lt;- max(dim(colours))\n    plot(\n      c(0, size),\n      c(0, -size),\n      type = \"n\",\n      xlab = \"\",\n      ylab = \"\",\n      axes = FALSE\n    )\n    rect(\n      col(colours) - 1,\n      -row(colours) + 1,\n      col(colours),\n      -row(colours),\n      col = colours,\n      border = borders\n    )\n    if (labels) {\n      label_col &lt;- contrast_text(colours,...)\n      text(col(colours) - 0.5,\n           -row(colours) + 0.5,\n           colours,\n           cex = cex_label,\n           col = label_col)\n    }\n  }\n\n\n\nlibrary(pals)\npar(bg=NULL)\n\nnamed list()\n\ncolors &lt;- sample(pals::polychrome(),size = 20)\ncolor_plot(colors,method=\"relative\")\n\n\n\n\nDemonstrating the contrast_text()\n\n\n\n\nWriting this example, I found some ideas for colouring on this discussion thread.\n\n\nAnalysing modified Rankin Scale (mRS) scores\nI believe, that the analysis of mRS scores is a key discipline in clinical stroke research, as this measure of functional outcome has been a favourite for major stroke trials.\nOverall, the mRS has been analysed in a dichotomised fashion with different cutoffs and using ordinal logistic regression. Most recently a new approach has gained some traction: “Tournament Methods”.(Churilov, Johns, and Turc, n.d.)\n\nwin_prob()\nThis approach has been implemented in the genodds-package(Johns 2022), and based on the work by Zou, Zou, and Choi (2022), I have created the win_prob() function:\n\nlibrary(stRoke)\nwin_prob(\n  data = stRoke::talos,\n  response = \"mrs_6\",\n  group = \"rtreat\",\n  sample.size = FALSE,\n  print.tables = FALSE\n)\n\n     Zou et al's winP (doi: 10.1161/STROKEAHA.121.037744) \n\nProbability of a random observation in Placebo group \n      will have a higher response score than a random\n      observation in Active group:\n\n       winP: 0.400 (0.612, 0.372)      p=0.0125\n--------------------------------------------\n\nThe numbers needed to treat (NNT) are: -9\n\n\n\n\ngeneric_stroke()\nThe author of the genodds-package is also maintaining the rankinPlot-package, which makes it very easy to create the classic Grotta Bars visualising changes in mRS score distributions.(Johns 2023)\nI have created the generic_stroke()-function to easily create table one, Grotta bars and binary or ordinal logistic regression plots with confidence intervals.\n\nlibrary(stRoke)\nlst &lt;- generic_stroke(df = talos,\n               \"rtreat\",\n               \"mrs_6\",\n               variables = c(\"hypertension\", \"diabetes\", \"civil\"))\n# names(lst)\n\nThe first element in the function is a classical table 1 created using the great package gtsummary(Sjoberg et al. 2021) using tbl_summary():\n\n\nPrinting Table 1\nlibrary(gtsummary)\nlst$`Table 1` |&gt; as_gt()\n\n\n\n\n\n\n  \n    \n    \n      Characteristic\n      Overall, N = 2001\n      Active, N = 791\n      Placebo, N = 1211\n    \n  \n  \n    hypertension\n101 (50%)\n38 (48%)\n63 (52%)\n    diabetes\n23 (12%)\n9 (11%)\n14 (12%)\n    civil\n\n\n\n        alone\n59 (30%)\n22 (28%)\n37 (31%)\n        partner\n141 (70%)\n57 (72%)\n84 (69%)\n  \n  \n  \n    \n      1 n (%)\n    \n  \n\n\n\n\nThe next element is a figure showcasing the grottaBar() function.\n\n\nPrinting Figure 1\nprint(lst$`Figure 1`)\n\n\n\n\n\n\n\n\n\nThe function has been expanded lately to allow further customisation. The generic_stroke() uses the default implementation.\nThe last element is the only original part of the implementation. This is a horizontal forest plot of regression analysis values from the ci_plot() function, which plots model coefficients with confidence intervals based on a supplied model. Supports binary and ordinal logistic regression and other functions formatted in a similar way.\n\n\nPrinting Figure 2\nprint(lst$`Figure 2`)\n\n\n\n\n\n\n\n\n\nThe output is a ggplot-element and is highly modifiable."
  },
  {
    "objectID": "health_data/REDCapCAST.html",
    "href": "health_data/REDCapCAST.html",
    "title": "Package: REDCapCAST",
    "section": "",
    "text": "In our group (and at most Danish public research institutions) we collect clinical data using REDCap.\nWe also use the “longitudinal project” option. This gives some issues with castellated data. Different approaches to handling this problem exists, but none that kept the focused data acquisition approach of the very nice REDCapR-library.\nThe REDCapCAST-library is based on the REDCapRITS-library, has been documented elsewhere, and is quickly approaching submission to CRAN.\nI am currently working on a handbook on using REDCap in R implementing the REDCapCAST-library. I hope to soon be able to release a first version, and until then, the work in progress can be followed."
  },
  {
    "objectID": "health_data/REDCapCAST.html#working-with-redcap-data",
    "href": "health_data/REDCapCAST.html#working-with-redcap-data",
    "title": "Package: REDCapCAST",
    "section": "",
    "text": "In our group (and at most Danish public research institutions) we collect clinical data using REDCap.\nWe also use the “longitudinal project” option. This gives some issues with castellated data. Different approaches to handling this problem exists, but none that kept the focused data acquisition approach of the very nice REDCapR-library.\nThe REDCapCAST-library is based on the REDCapRITS-library, has been documented elsewhere, and is quickly approaching submission to CRAN.\nI am currently working on a handbook on using REDCap in R implementing the REDCapCAST-library. I hope to soon be able to release a first version, and until then, the work in progress can be followed."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hello world!"
  },
  {
    "objectID": "academia.html",
    "href": "academia.html",
    "title": "Research results",
    "section": "",
    "text": "I want to share the progress of the research I am involved with. All of this work have been in corporation with my great colleagues and supervisors to who I am very grateful. I will be sharing publications, code, posters and other small project along the way.\nI have published most of my coding to GitHub (or my private Gitea instance) already, but I want to share the thoughts behind as well.\nYou are welcome to comment or use any ideas shared.\n\n\n\n\n\n\n\n\n  \n\n\n\n\nESOC 2023: Predicting physical activity level after stroke\n\n\n\n\n\n\n\nConference abstract\n\n\nPhysical activity\n\n\nElastic net\n\n\nR\n\n\n\n\nA little background to our abstract presented at the ESOC 2023 conference in Munich.\n\n\n\n\n\n\nMay 3, 2023\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "academia/predict_pa.html",
    "href": "academia/predict_pa.html",
    "title": "ESOC 2023: Predicting physical activity level after stroke",
    "section": "",
    "text": "I am presenting a poster at European Stroke Organisation Conference 2023 on predicting changes in physical activity after stroke.\nThe poster will be part of the poster viewing programme on Wednesday, May 24 2023."
  },
  {
    "objectID": "academia/predict_pa.html#intro",
    "href": "academia/predict_pa.html#intro",
    "title": "ESOC 2023: Predicting physical activity level after stroke",
    "section": "",
    "text": "I am presenting a poster at European Stroke Organisation Conference 2023 on predicting changes in physical activity after stroke.\nThe poster will be part of the poster viewing programme on Wednesday, May 24 2023."
  },
  {
    "objectID": "academia/predict_pa.html#about-the-poster",
    "href": "academia/predict_pa.html#about-the-poster",
    "title": "ESOC 2023: Predicting physical activity level after stroke",
    "section": "About the poster",
    "text": "About the poster\nSee the poster here.\nI wanted to divert from the traditional text heavy poster format. For the session at ESOC, I will be at the poster stand for most of the time to talk about our work. The abstract will be available for download for the participants.\nThe poster is created in PowerPoint, as this was where I had an available template. The template was later abandoned though. The font used is the free and open source font Jost*. Inspired by German design tradition. Icons are from the Material Design Icons, and also open source."
  },
  {
    "objectID": "academia/predict_pa.html#background",
    "href": "academia/predict_pa.html#background",
    "title": "ESOC 2023: Predicting physical activity level after stroke",
    "section": "Background",
    "text": "Background\nPhysical activity (PA) reduces the risk of stroke and improves functional outcome. We aimed to investigate predictors for decrease and increase in PA after stroke. We have been interested in trying to predict patients at increased risk of physical activity decline after stroke."
  },
  {
    "objectID": "academia/predict_pa.html#methods",
    "href": "academia/predict_pa.html#methods",
    "title": "ESOC 2023: Predicting physical activity level after stroke",
    "section": "Methods",
    "text": "Methods\nAll analysis were performed using R and RStudio. We used the elastic net regression model as implemented in the glmnet-package for R.1\nI have used the great book “An introduction to statistical learning with applications in R”.(James et al. 2021) This book is freely available and the authors have even created small talks on each chapter (though only for the first edition). I believe this book is the main curriculum for beginning work with statistical learning (or machine learning, but that matter).\nThe script used for creating a regularised prediction model is below.\n\n\nOptimisation and regularisation steps\n## ====================================================================\n## Step 0: data import and wrangling\n## ====================================================================\n\n# source(\"data_format.R\")\ny1&lt;-factor(as.integer(y)-1) ## Outcome is required to be factor of 0 or 1.\n\n\n## ====================================================================\n## Step 1: settings\n## ====================================================================\n\n## Folds\nK=10\nset.seed(3)\nc&lt;-caret::createFolds(y=y, \n                      k = K, \n                      list = FALSE, \n                      returnTrain = TRUE) # Foldids for alpha tuning\n\n## Defining tuning parameters\nlambdas=2^seq(-10, 5, 1)\nalphas&lt;-seq(0,1,.1)\n\n## Weights for models\nweighted=TRUE\nif (weighted == TRUE) {\n  wght&lt;-as.vector(1 - (table(y)[y] / length(y)))\n} else {\n  wght &lt;- rep(1, nrow(y))\n}\n\n\n## Standardise numeric\n## Centered and \n\n\n\n## ====================================================================\n## Step 2: all cross validations for each alpha\n## ====================================================================\n\nlibrary(furrr)\nlibrary(purrr)\nlibrary(doMC)\nregisterDoMC(cores=6)\n\n# Nested CVs with analysis for all lambdas for each alpha\n# \nset.seed(3)\ncvs &lt;- future_map(alphas, function(a){\n  cv.glmnet(model.matrix(~.-1,X),\n            y1,\n            weights = wght,\n            lambda=lambdas, \n            type.measure = \"deviance\", # This is standard measure and recommended for tuning\n            foldid = c, # Per recommendation the folds are kept for alpha optimisation\n            alpha=a,\n            standardize=TRUE,\n            family=quasibinomial,\n            keep=TRUE) # Same as binomial, but not as picky\n})\n\n## ====================================================================\n# Step 3: optimum lambda for each alpha\n## ====================================================================\n\n\n# For each alpha, lambda is chosen for the lowest meassure (deviance)\neach_alpha &lt;- sapply(seq_along(alphas), function(id) {\n  each_cv &lt;- cvs[[id]]\n  alpha_val &lt;- alphas[id]\n  index_lmin &lt;- match(each_cv$lambda.min, \n                      each_cv$lambda)\n  c(lamb = each_cv$lambda.min, \n    alph = alpha_val,\n    cvm = each_cv$cvm[index_lmin])\n})\n\n# Best lambda\nbest_lamb &lt;- min(each_alpha[\"lamb\", ])\n\n# Alpha is chosen for best lambda with lowest model deviance, each_alpha[\"cvm\",]\nbest_alph &lt;- each_alpha[\"alph\",][each_alpha[\"cvm\",]==min(each_alpha[\"cvm\",]\n                                                         [each_alpha[\"lamb\",] %in% best_lamb])]\n\n## https://stackoverflow.com/questions/42007313/plot-an-roc-curve-in-r-with-ggplot2\np_roc&lt;-roc.glmnet(cvs[[1]]$fit.preval, newy = y)[[match(best_alph,alphas)]]|&gt; # Plots performance from model with best alpha\n  ggplot(aes(FPR,TPR)) + \n  geom_step() +\n  coord_cartesian(xlim=c(0,1), ylim=c(0,1)) +\n  geom_abline()+\n  theme_bw()\n\n## ====================================================================\n# Step 4: Creating the final model\n## ====================================================================\n\nsource(\"regular_fun.R\") # Custom function\noptimised_model&lt;-regular_fun(X,y1,K,lambdas=best_lamb,alpha=best_alph) \n# With lambda and alpha specified, the function is just a k-fold cross-validation wrapper, \n# but keeps model performance figures from each fold.\n\nlist2env(optimised_model,.GlobalEnv)\n# Function outputs a list, which is unwrapped to Env.\n# See source script for reference.\n\n## ====================================================================\n# Step 5: creating table of coefficients for inference\n## ====================================================================\n\nBmatrix&lt;-matrix(unlist(B),ncol=10)\nBmedian&lt;-apply(Bmatrix,1,median)\nBmean&lt;-apply(Bmatrix,1,mean)\n\nreg_coef_tbl&lt;-tibble(\n  name = c(\"Intercept\",Hmisc::label(X)),\n  medianX = round(Bmedian,5),\n  ORmed = round(exp(Bmedian),5),\n  meanX = round(Bmean,5),\n  ORmea = round(exp(Bmean),5))%&gt;%\n  # arrange(desc(abs(medianX)))%&gt;%\n  gt()\n\n## ====================================================================\n# Step 6: plotting predictive performance\n## ====================================================================\n\nreg_cfm&lt;-confusionMatrix(cMatTest)\nreg_auc_sum&lt;-summary(auc_test[,1])\n\n## ====================================================================\n# Step 7: Packing list to save in loop\n## ====================================================================\n\nls[[i]] &lt;- list(\"RegularisedCoefs\"=reg_coef_tbl,\n                \"bestA\"=best_alph,\n                \"bestL\"=best_lamb,\n                \"ConfusionMatrx\"=reg_cfm,\n                \"AUROC\"=reg_auc_sum)"
  },
  {
    "objectID": "academia/predict_pa.html#publication-status",
    "href": "academia/predict_pa.html#publication-status",
    "title": "ESOC 2023: Predicting physical activity level after stroke",
    "section": "Publication status",
    "text": "Publication status\nWe have recently applied for additional registry based data on socio economic status to include in the analysis. We are awaiting this data before publishing our main article on this project."
  },
  {
    "objectID": "academia/predict_pa.html#footnotes",
    "href": "academia/predict_pa.html#footnotes",
    "title": "ESOC 2023: Predicting physical activity level after stroke",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nVersions of glmnet also exists for MATLAB and Python↩︎"
  },
  {
    "objectID": "health-data.html",
    "href": "health-data.html",
    "title": "Health Data projects",
    "section": "",
    "text": "During my work in clinical research I have acquired a special set of skills in working with data collection, data bases and data analysis. I am very much interested in sharing these skils and knowledge.\nI am primarily working with R and REDCap. I have published one R package to CRAN and am actively maintaining two other packages on GitHub (on is on its way to CRAN). I am also working on a handbook for clinical researchers on working with R and REDCap.\nI have worked as a data science consultant on a few projects. Here I will also share a few of them as a portfolio. Please don’t hesitate to contact me if you need help on a project.\nI also just want to underline, that I am by no means a software engineer, but a medical doctor with a great interest in data science. No coding shared here comes with no guarantees, and I will not be liable to any errors.\n\n\n\n\n\n\n\n\n  \n\n\n\n\nPackage: stRoke\n\n\n\n\n\n\n\nREDCap\n\n\nPackage\n\n\nR\n\n\nFOSS\n\n\n\n\nCollection of tools used in stroke research, but widely applicable.\n\n\n\n\n\n\nMay 17, 2023\n\n\n\n\n\n\n  \n\n\n\n\nPackage: REDCapCAST\n\n\n\n\n\n\n\nREDCap\n\n\nPackage\n\n\nR\n\n\nFOSS\n\n\n\n\nCasting data bases and handling castellated data sets.\n\n\n\n\n\n\nMay 3, 2023\n\n\n\n\n\n\n  \n\n\n\n\nProject: Foot database\n\n\n\n\n\n\n\nREDCap\n\n\nProject\n\n\nR\n\n\nConsultant\n\n\n\n\nComprehensive data and data base migration to continue data collection.\n\n\n\n\n\n\nApr 28, 2023\n\n\n\n\n\n\nNo matching items"
  }
]